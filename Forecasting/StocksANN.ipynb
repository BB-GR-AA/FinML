{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>One-day Stock Price Forecasting Using Deep Neural Netrowks</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table of Contents</h1>\n",
    "\n",
    "<ul>\n",
    "    <li><a href=\"#Description\">Problem Description</a></li>\n",
    "    <li><a href=\"#Data Preparation\">Data Preparation</a></li>\n",
    "    <li><a href=\"#Model\">Model</a></li>\n",
    "    <li><a href=\"#Train\">Training</a></li>\n",
    "    <li><a href=\"#Results\">Results</a></li>\n",
    "    <li><a href=\"#Source\">Source code</a></li>\n",
    "</ul>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 href=\"#Problem Description\">Problem Description</h2>\n",
    "\n",
    "Deep Learning neural networks are capable of modeling a process by learning from a given dataset. That is, a neural network is a function $\\hat{y}=f(x|W)$ that can be used to approximate most fucntions using a set of trainable parameters.\n",
    "\n",
    "If a given neural netrowk can be trained to mimic the behaviour of some time series $s(t)$ we can use such network to make predictions for a given time. More formally, we want to perform univariate one-step forecasting on a dataset of daily close prices to capture the behaviour of a particular stock.\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{s}(t) = f(s(t-1),s(t-2),...,s(t-l)|W)\\\\\n",
    "\\end{align}\n",
    "\n",
    "Here, $W$ is the set of trainable weights that are used inside the network to map a feature vector to a target (input to output). We represent the daily close prices as follows,\n",
    "\n",
    "\\begin{align}\n",
    "d = \\{(\\mathbf{x}_i,y_i)\\}^{n-1}_{i=0}\\\\\n",
    "\\end{align}\n",
    "\n",
    "The dataset has $n$ tuples $(\\mathbf{x}_i,y_i)$ representing the $\\textit{i-th}$ feature vector $\\mathbf{x}_i=[s(t-1), s(t-2), ..., s(t-l)]$ and the scalar training sample $y_i=s(t)$. The parameter $l$ controls the number of past samples included in the forecast.\n",
    "\n",
    "Note that $d$ is not the original time series $s(t)$, we first have to re-frame the raw data set to one suitable for a supervised learning problem by transforming the univariate time series into a multivariate one via time delay embedding,\n",
    "\n",
    "\\begin{equation}\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "s_0 & s_1 & s_2 & \\cdots & s_{l-1}\\\\\n",
    "s_1 & s_2 & s_3 &\\cdots & s_{l}\\\\\n",
    "s_2 & s_3 & s_4 & \\cdots & s_{l+1}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "s_{m-l} & s_{m-l+1} & s_{m-l+2} & \\cdots & s_{m-1}\\\\\n",
    "\\end{bmatrix} \n",
    "\\end{equation}\n",
    "\n",
    "Here, $m$ is the last sample in the original data set and the feature vectors are the rows of $X$. Simmlarly, the targets are the following scalar entries,\n",
    "\n",
    "\\begin{equation}\n",
    "Y = \n",
    "\\begin{bmatrix}\n",
    "s_l\\\\\n",
    "s_{l+1}\\\\\n",
    "s_{l+2}\\\\\n",
    "\\vdots\\\\\n",
    "s_{m}\n",
    "\\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 href=\"#Data Preparation\">Data Preparation</h2>\n",
    "\n",
    "We need to make the supervised learning dataset and create a Dataset class to work with DataLoader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class DataSupervised(Dataset):\n",
    "    '''\n",
    "        Custom dataset to work with DataLoader for supervised learning.\n",
    "        \n",
    "        file_name (str): Path to csv file.\n",
    "        target_cols (int): Number of steps (forecasts), one by default (last column).\n",
    "        train (bool): Train (odd samples) or test split (even samples), True by default.\n",
    "    '''    \n",
    "    \n",
    "    def __init__(self, file_name, target_cols=1, train=True):\n",
    "        \n",
    "        stock_supervised = pd.read_csv(file_name).values\n",
    "        X_train, X_test = train_test_split(stock_supervised,test_size=0.2)         \n",
    "        if train:\n",
    "            self.X = torch.FloatTensor(X_train[:,:-target_cols])\n",
    "            self.Y = torch.FloatTensor(X_train[:,-target_cols])\n",
    "            if target_cols == 1:\n",
    "                self.Y = self.Y.unsqueeze(1)            \n",
    "        else:\n",
    "            self.X = torch.FloatTensor(X_test[:,:-target_cols])     \n",
    "            self.Y = torch.FloatTensor(X_test[:,-target_cols])\n",
    "            if target_cols == 1:\n",
    "                self.Y = self.Y.unsqueeze(1)                   \n",
    "            \n",
    "        self.n_samples = self.X.shape[0]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.Y[index] \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 href=\"#Model\">Model</h2>\n",
    "\n",
    "One of the simplest architectures for a multi-dimensional inputs is the [multi-layer perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron). It is composed of an input layer followed by a variable number of fully connected layers and nodes, and the output layer.\n",
    "\n",
    "<p>The number of neurons in the input layer corresponds to the number of features the model uses, while the number of nodes in the output layer tells us how many samples we are forecasting. The number of neurons in the hidden layer translates to the complexity of the model. A model with too many hidden neurons could result in a model that is too complex for the dataset and thus lead to overfitting. In contrast, a model with too few hidden neurons can fail to capture the complexity of the data and result in underfitting.</p>\n",
    "\n",
    "<p>We define our neural network as a class that allows for custom number of hidden layers, number of neurons per hidden layer, and choice of non-linear transformation in the hidden layer.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class ANN(nn.Module):\n",
    "    \"\"\"Artificial Neural Network\n",
    "    \n",
    "        MLP with tanh activation function for the hidden layers and linear transformation\n",
    "        for the output layer by default.\n",
    "        \n",
    "        Layers -- (list) Numbers of neurons in each layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, Layers):\n",
    "        super(ANN, self).__init__()  \n",
    "        self.hidden = nn.ModuleList()\n",
    "        \n",
    "        for input_size, output_size in zip(Layers, Layers[1:]):\n",
    "            linear = nn.Linear(input_size, output_size)\n",
    "            self.hidden.append(linear)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        layers = len(self.hidden)\n",
    "        for (layer, linear_transform) in zip(range(layers), self.hidden):\n",
    "            if layer < layers - 1:\n",
    "                x = torch.tanh(linear_transform(x))\n",
    "            else:\n",
    "                x = linear_transform(x)\n",
    "        return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![vanilla_MLP](figures/stocks_vanilla_MLP.png)\n",
    "*Multi-dimensional input fully connected neural network with $p$ perceptrons or neurons in a single hidden layer. The nodes represent the inputs, activations or outputs and the edges represent the weights and biases. The biases are assumed to be zero for simplicity. The yellow circles represent a linear transformation while the blue rectangle represents a non-linear transformation (activation function).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 href=\"#Train\">Training</h2>\n",
    "\n",
    "To train the network there are a couple of things we need to define:\n",
    "* The loss function\n",
    "* How the data gets fed to the network\n",
    "\n",
    "<p> we are modeling a time-series via non-linear regression, thus using the mean squared error (MSE) is a good starting point to measure the performance of the network.</p>\n",
    "\n",
    "\\begin{align}\n",
    "\\mathit{MSE}:=\\frac{1}{n} \\sum_{i=0}^{n-1} (\\mathbf{y}_{i}-\\hat{\\mathbf{y}}_{i})^2 \n",
    "\\end{align}\n",
    "\n",
    "<p>Where $\\hat{\\mathbf{y}}_{i}$ is the i-th estimated value. Notice that the definition above averages over all training samples for a given epoch.</p>\n",
    "\n",
    "<p>For very large datasets it is sometimes conveient to feed the training samples in batches to avoid computing the gradient over all samples. This approximationg approaches the true gradient as the batch size tends to the whole dataset. As a consequence the MSE is computing for a given batch.</p>\n",
    "\n",
    "<p>Lastly, we shuffle the training samples at each epoc to increase the robustness of the model. This ensures that the model is not fed the same batch at later epochs</p>\n",
    "\n",
    "[What is batch size in neural networks](https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network) <br>\n",
    "[Why should the data be shuffled for machine learning tasks](https://datascience.stackexchange.com/questions/24511/why-should-the-data-be-shuffled-for-machine-learning-tasks) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def train_series(train_loader, model, criterion, optimizer, epochs=10, test_loader=None, display_batch=False):\n",
    "    \"\"\"Train and test (optional) a time-series model, returns the loss at a given epoch.\n",
    "\n",
    "    train_loader -- DataLoader object with the training dataset.\n",
    "    model -- Neural Network to be trained.\n",
    "    criterion -- Loss function.\n",
    "    optimizer -- optimization algorithm to update the network weights.\n",
    "    epochs -- Number of forward and backward passes on the whole dataset.\n",
    "    test_loader -- DataLoader object with the test dataset (default None).\n",
    "    display_batch -- Display epoch, bactch index and batch length (default False).\n",
    "    \"\"\"\n",
    "    \n",
    "    model.train()\n",
    "    results = {'training loss': [], 'validation error': []}   # loss at a given epoch        \n",
    "    for epoch in range(epochs):\n",
    "         \n",
    "        total = 0 # training loss for every epoch        \n",
    "        for batch_idx, (x, y) in enumerate(train_loader):             \n",
    "            if display_batch: \n",
    "              print('epoch {}, batch idx {} , batch len {}'.format(epoch, batch_idx, len(y)))              \n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(x), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total += loss.item()  # cummulative loss/batch\n",
    "        results['training loss'].append(total/batch_idx) # ~ loss over all training samples\n",
    "        \n",
    "        if test_loader is not None:\n",
    "            results['validation error'].append(test_series(test_loader, model, criterion))\n",
    "        \n",
    "    return results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define the auxiliary method for computing the error on a test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def test_series(test_loader, model, criterion):\n",
    "    \"\"\" test a a time-series model, returns the error.\n",
    "    \n",
    "    test_loader -- DataLoader object with the test dataset.\n",
    "    model -- Neural Network to be evaaluated.\n",
    "    criterion -- Loss function.\n",
    "    \"\"\"        \n",
    "    \n",
    "    model.eval()\n",
    "    error = 0   \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x, y) in enumerate(test_loader):\n",
    "            error += criterion(model(x), y).item()   # cummulative error/batch\n",
    "    model.train()\n",
    "\n",
    "    return error / batch_idx # ~ error over all testing samples\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 href=\"#Results\">Results</h2>\n",
    "\n",
    "We test the model on the [chart](https://finance.yahoo.com/quote/ANET/?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&guce_referrer_sig=AQAAACcYE76pxfIMIu7x_2P-BSD9ufvEL1Gjz1_QKIR6uRffsIMUP_xzVzuOGJusIHMIbajTyvL-8LbgoetbBKZ1Cl6TfQToIJ5gV4x3DG9aOT3lALbP59gjIJZmmihz52uicQWfUuMDu5jnlMXgH4t3BXl_CduQBEgS-FzwuyFF_4vd) for [Arista Networks](https://www.arista.com/en/) (ANET) for the dates of 06/06/2014 to 08/18/2020. The train/test split follows an 80/20 split to mimic a production setting where we wish to predict prices in the future. Splitting the data randomly can lead to look-ahead bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "model = ANN(Layers=[in_features, 10, 10, out_features])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Result](figures/train_test_error_1_10_10_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Candles Plot of stock\n",
    "* Plot of estimated values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 href=\"#Source\">Source code</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Python script](https://github.com/Randomized-Neurais-Learners/FinML/blob/master/Forecasting/StocksANN.py)\n",
    "* [Data request](https://github.com/Randomized-Neurais-Learners/FinML/blob/master/Data/getHistoricalDaily.py)\n",
    "* [Data preparation](https://github.com/Randomized-Neurais-Learners/FinML/blob/master/Data/series_to_supervised.py)\n",
    "* [FinML module](https://github.com/Randomized-Neurais-Learners/FinML/blob/master/FinML.py)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
